{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Porting an analysis from local to distributed\n",
    "\n",
    "In this pair of notebooks, we will develop a prototype unsupervised analysis with sklearn and then port it to use Spark, as we might in real life.\n",
    "\n",
    "<a href = \"http://yogen.io\"><img src=\"http://yogen.io/assets/logo.svg\" alt=\"yogen\" style=\"width: 200px; float: right;\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Porting our analysis to a distributed setting using Spark DataFrames\n",
    "\n",
    "First, read the input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can specify `.read` options in two equivalent ways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract KPIs\n",
    "\n",
    "A direct transliteration of what we did with sklearn: columnwise arithmetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning in Spark\n",
    "\n",
    "### `spark.mllib` and `spark.ml`\n",
    "\n",
    "`spark.mllib` was the original Spark Machine Learning package. It is based on RDDs. It's now on maintenance mode.\n",
    "\n",
    "Since the focus on DataFrames, `spark.ml` is the primary ML package. It's designed around DataFrames. \n",
    "\n",
    "Somewhat confusingly, \"Spark MLLIB\" is the name of the library, and it comprises both.\n",
    "\n",
    "https://spark.apache.org/docs/2.2.1/ml-guide.html\n",
    "https://spark.apache.org/docs/2.2.1/api/python/pyspark.ml.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformers, Estimators and Models\n",
    "\n",
    "In Spark ML, a Transformer is an object that represents a transformation from a DataFrame into another DataFrame.\n",
    "\n",
    "Many preprocessing and feature extraction steps are represented by Transformers. In general, this happens when they don't have to learn anything from the dataset itself, which means they implement the `transform` but not the `fit` method.\n",
    "\n",
    "When the transformation does depend on the dataset, the transformation is represented by an Estimator. An Estimator implements a `fit` method that generates a Model, which is a Transformer.\n",
    "\n",
    "All Machine Learning models are Estimators, and many feature extraction steps are as well.\n",
    "\n",
    "https://spark.apache.org/docs/2.2.0/ml-pipeline.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assemble features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training our model\n",
    "\n",
    "Just like in sklearn, we use a `.fit` method. The difference is that Estimators are stateless: the software object we call kmeans doesn't learn, that is, it doesn't change its state like sklearn models do (remember the underscore-suffixed attributes of sklearn models, which are generated from the data). \n",
    "\n",
    "What it does is generate a new object, the one we call model, which is a Transformer. This object contains the information necessary to `.transform` our data, which is the method name spark.ml uses in place of `.predict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assigning clusters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Z scores\n",
    "\n",
    "Now we only need to calculate a z-score, so that we can score points as outliers or not outliers.\n",
    "\n",
    "For that, we need the per-cluster mean and stddev for each of the variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group by cluster and extract mean and stddev for each revenue and tax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the aggregated results with the original DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we are ready to do the arithmetic!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the results\n",
    "\n",
    "We use the standard Python scientific stack to plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks just the same!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix: saving models\n",
    "\n",
    "Just like in sklearn, we can save learned models so that we can share, import or deploy them. In this case we have an additional perk over sklearn: we can use the saved models in pyspark, but also in Scala or Java-based Spark applications. R is trickier, but it should be coming at some point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Turn this analysis into a Spark job (you can find an example in notebook 03).\n",
    "\n",
    "Upload it to the cluster and run it."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
